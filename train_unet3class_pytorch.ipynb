{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as tF\n",
    "import torchvision.transforms.functional as tvF\n",
    "\n",
    "import random\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.backends.cudnn as cudnn\n",
    "from easydict import EasyDict\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import time\n",
    "from collections import Counter\n",
    "import gc\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSIONS = ['.jpg', '.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIONS = EasyDict()\n",
    "OPTIONS.DEBUG = False # Less data to train and test\n",
    "OPTIONS.CODE_NAME = 'UNET_3class_64_DO20' + ('_debug' if OPTIONS.DEBUG else '')\n",
    "OPTIONS.NUM_CLASSES = 3\n",
    "\n",
    "OPTIONS.LOG = EasyDict()\n",
    "OPTIONS.LOG.LOG_FILE = '/home/kevin/nuclei_segmentation/log/log_{}.txt'.format(OPTIONS.CODE_NAME)\n",
    "\n",
    "OPTIONS.TRAIN = EasyDict()\n",
    "OPTIONS.TRAIN.BATCH_SIZE = 8\n",
    "OPTIONS.TRAIN.SHUFFLE = True\n",
    "OPTIONS.TRAIN.IMG_DIR = './train_aug_with_tumor2/imgs/train/'\n",
    "OPTIONS.TRAIN.MASK_DIR = './train_aug_with_tumor2/masks/train/'\n",
    "OPTIONS.TRAIN.PRINT_FREQ = 50\n",
    "OPTIONS.TRAIN.LR = 1e-4\n",
    "OPTIONS.TRAIN.LR_DECAY_GAMMA = 0.5\n",
    "OPTIONS.TRAIN.LR_DECAY_MILESTONES = [3, 7, 11, 15, 19]\n",
    "OPTIONS.TRAIN.MAX_EPOCH = 20\n",
    "OPTIONS.TRAIN.EPITHELIUM_WEIGHT = None #set to None if you don't need to weight the crossentropy.\n",
    "\n",
    "OPTIONS.VAL = EasyDict()\n",
    "OPTIONS.VAL.SPLIT_RND_SEED = 1357\n",
    "OPTIONS.VAL.RATIO = 0.20\n",
    "OPTIONS.VAL.BATCH_SIZE = 16\n",
    "OPTIONS.VAL.SHUFFLE = False\n",
    "OPTIONS.VAL.IMG_DIR = OPTIONS.TRAIN.IMG_DIR\n",
    "OPTIONS.VAL.MASK_DIR = OPTIONS.TRAIN.MASK_DIR\n",
    "\n",
    "\n",
    "OPTIONS.DATA = EasyDict()\n",
    "OPTIONS.DATA.INPUT_SIZE = 256\n",
    "\n",
    "OPTIONS.CHECKPOINT = EasyDict()\n",
    "OPTIONS.CHECKPOINT.DIR = '/home/kevin/nuclei_segmentation/checkpoints/checkpoints_{}'.format(OPTIONS.CODE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(OPTIONS.CODE_NAME)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "\n",
    "log_file = logging.FileHandler(OPTIONS.LOG.LOG_FILE)\n",
    "log_file.setLevel(logging.DEBUG)\n",
    "\n",
    "fmt = '%(asctime)s %(levelname)-8s: %(message)s'\n",
    "fmt = logging.Formatter(fmt)\n",
    "\n",
    "log_file.setFormatter(fmt)\n",
    "logger.addHandler(log_file)\n",
    "\n",
    "logger.info('\\n\\n'+str(OPTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_info(msg):\n",
    "    print(msg)\n",
    "    logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Path to directory with images.\n",
    "            mask_dir (string): Path to directory with masks.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        assert len(glob.glob(os.path.join(img_dir, '*'))) == len(glob.glob(os.path.join(mask_dir, '*')))\n",
    "        self.debug = debug\n",
    "        if self.debug:\n",
    "            self.img_dir = img_dir\n",
    "            self.img_names = glob.glob(os.path.join(self.img_dir, '*'))\n",
    "            random.shuffle(self.img_names)\n",
    "            self.img_names = self.img_names[:4096]\n",
    "            self.mask_dir = mask_dir\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.img_dir = img_dir\n",
    "            self.img_names = glob.glob(os.path.join(self.img_dir, '*'))\n",
    "            self.mask_dir = mask_dir\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.debug:\n",
    "            return len(self.img_names)\n",
    "        else:\n",
    "            return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        mask_name = os.path.join(self.mask_dir,self.img_names[idx].split('/')[-1].split('.')[0] + '_mask.png')\n",
    "        image = io.imread(img_name)\n",
    "        mask = io.imread(mask_name)\n",
    "        sample = {'image': image, 'mask': mask}#\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomResizedCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, scale, interpolation=PIL.Image.NEAREST):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "            self.scale = scale\n",
    "            self.interpolation = interpolation\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "            self.scale = scale\n",
    "            self.interpolation = interpolation\n",
    "        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        for attempt in range(10):\n",
    "            area = image.shape[0] * image.shape[1]\n",
    "            target_area= random.uniform(*self.scale) * area\n",
    "            w = int(round(math.sqrt(target_area)))\n",
    "            h = w\n",
    "            if w <= image.shape[0] and h <= image.shape[1]:\n",
    "                top = random.randint(0, image.shape[0] - h)\n",
    "                left = random.randint(0, image.shape[1] - w)\n",
    "                image = image[top:top+h,\n",
    "                              left:left+w]\n",
    "                mask = mask[top:top+h,\n",
    "                            left:left+w]\n",
    "\n",
    "                image = PIL.Image.fromarray(image).resize(self.output_size, self.interpolation)\n",
    "                mask  = PIL.Image.fromarray(mask).resize(self.output_size, self.interpolation)\n",
    "        \n",
    "                return {'image': image, 'mask': mask}\n",
    "\n",
    "class RandomRot90(object):\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly rotated image.\n",
    "        \"\"\"\n",
    "\n",
    "        t = random.choice([None, PIL.Image.ROTATE_90, PIL.Image.ROTATE_180, PIL.Image.ROTATE_270])\n",
    "        \n",
    "        if t is None:\n",
    "            return sample\n",
    "        else:\n",
    "            return {'image': sample['image'].transpose(t),\n",
    "                    'mask' : sample['mask'].transpose(t)}\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class ColorJitter(transforms.ColorJitter):\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Input image.\n",
    "        Returns:\n",
    "            PIL Image: Color jittered image.\n",
    "        \"\"\"\n",
    "        transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "        return {'image': transform(sample['image']),\n",
    "                'mask': sample['mask']}\n",
    "\n",
    "class RandomHorizontalFlip(transforms.RandomHorizontalFlip):\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return {'image': tvF.hflip(sample['image']),\n",
    "                    'mask': tvF.hflip(sample['mask'])}\n",
    "        return sample\n",
    "\n",
    "class RandomVerticalFlip(transforms.RandomVerticalFlip):\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return {'image': tvF.vflip(sample['image']),\n",
    "                    'mask': tvF.vflip(sample['mask'])}\n",
    "        return sample\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    .. note::\n",
    "        This transform acts in-place, i.e., it mutates the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return {'image': transforms.functional.normalize(sample['image'], self.mean, self.std),\n",
    "                'mask' : sample['mask']}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "class NormalizeTest(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    .. note::\n",
    "        This transform acts in-place, i.e., it mutates the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return {'image_ori': sample['image'],\n",
    "                'image': transforms.functional.normalize(sample['image'], self.mean, self.std),\n",
    "                'mask' : sample['mask'],\n",
    "                }\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert PIL images and masks in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = np.array(image)/255.\n",
    "        mask = np.array(mask)/255.\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "        mask[mask<0.5] = 0.\n",
    "        mask[mask>0.5] = 1.\n",
    "        image = image.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'mask': torch.from_numpy(mask)}\n",
    "    \n",
    "class ToTensorTest(object):\n",
    "    \"\"\"Convert PIL images and masks in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = np.array(image)/255.\n",
    "        mask = np.array(mask)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "#        mask = mask.transpose((2, 0, 1))\n",
    "#        mask[mask<0.5] = 0.\n",
    "#        mask[mask>0.5] = 1.\n",
    "        image = image.astype(np.float32)\n",
    "#        mask = mask.astype(np.float32)\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'mask': torch.from_numpy(mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                      std=[0.229, 0.224, 0.225])\n",
    "train_dataset = MyDataset(OPTIONS.TRAIN.IMG_DIR,\n",
    "                          OPTIONS.TRAIN.MASK_DIR,\n",
    "                          transform=transforms.Compose([\n",
    "                                    RandomResizedCrop(OPTIONS.DATA.INPUT_SIZE, scale=(0.8, 1.0)),\n",
    "                                    RandomRot90(),\n",
    "                                    ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "                                    RandomHorizontalFlip(),\n",
    "                                    RandomVerticalFlip(),\n",
    "                                    ToTensor(),\n",
    "                                    normalize]),\n",
    "                          debug=OPTIONS.DEBUG)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MyDataset(OPTIONS.VAL.IMG_DIR,\n",
    "                        OPTIONS.VAL.MASK_DIR,\n",
    "                          transform=transforms.Compose([\n",
    "                                    RandomResizedCrop(OPTIONS.DATA.INPUT_SIZE, scale=(0.8, 1.0)),\n",
    "                                    RandomRot90(),\n",
    "                                    ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "                                    RandomHorizontalFlip(),\n",
    "                                    RandomVerticalFlip(),\n",
    "                                    ToTensor(),\n",
    "                                    normalize]),\n",
    "                        debug=OPTIONS.DEBUG)\n",
    "\n",
    "random.shuffle(val_dataset.img_names)\n",
    "val_dataset.img_names = val_dataset.img_names[:int(len(val_dataset)*OPTIONS.VAL.RATIO)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=OPTIONS.TRAIN.BATCH_SIZE, shuffle=OPTIONS.TRAIN.SHUFFLE,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=OPTIONS.VAL.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "TOTAL: 25000 images\n"
     ]
    }
   ],
   "source": [
    "log_info('Training set:')\n",
    "cnt = Counter()\n",
    "log_info('TOTAL: {} images'.format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set:\n",
      "TOTAL: 5000 images\n"
     ]
    }
   ],
   "source": [
    "log_info('Val set:')\n",
    "log_info('TOTAL: {} images'.format(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, target):\n",
    "    \"\"\"Computes the accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = output.size(0)\n",
    "        img_area = output.size(2) * output.size(3)\n",
    "        pred = tF.softmax(output).argmax(dim=1)\n",
    "        correct = pred.eq(target).sum()\n",
    "        return float(correct) / (float(batch_size) * float(img_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_rec(cm):\n",
    "    pr_mat = np.zeros((len(classes), 2))\n",
    "    for i in range(len(classes)):\n",
    "        pr_mat[i, 0] = cm[i, i] / cm[:, i].sum()\n",
    "        pr_mat[i, 1] = cm[i, i] / cm[i, :].sum()\n",
    "        \n",
    "    pr_mat = pd.DataFrame(pr_mat)\n",
    "    pr_mat.columns = ['Precision', 'Recall']\n",
    "    pr_mat.index = classes\n",
    "    return pr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    # update parameters\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        image= sample['image']\n",
    "        mask=sample['mask'].type(torch.LongTensor)\n",
    "        n_samples = image.size(0)\n",
    "        \n",
    "        # need to create labels to match dimensions\n",
    "        labels = torch.argmax(mask, dim=1)\n",
    "        labels = labels.cuda()\n",
    "        image = image.cuda()\n",
    "        mask = mask.cuda()\n",
    "\n",
    "        output = model(image)\n",
    "        output = output.cuda()\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        accuracy = get_accuracy(output, labels)\n",
    "\n",
    "        losses.update(loss.item(), n_samples)\n",
    "        accuracies.update(accuracy)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        del labels, image, mask\n",
    "        gc.collect()\n",
    "        if i % OPTIONS.TRAIN.PRINT_FREQ == 0:\n",
    "            msg = 'Epoch: [{}][{}/{}]\\t'.format(epoch, i, len(train_loader))        \n",
    "            msg += 'Time: {:.3f} ({:.3f})\\tData: {:.3f}'.format(batch_time.val, batch_time.avg, data_time.val)\n",
    "            msg += '\\tAccuracy: {:.3f}\\t'.format(accuracies.val)\n",
    "            msg += '\\tLoss: {:.06f} ({:0.6f})'.format(losses.val, losses.avg)\n",
    "            log_info(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    confusion_mat = np.zeros((OPTIONS.NUM_CLASSES, OPTIONS.NUM_CLASSES), dtype=np.int)\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        \n",
    "        for i, sample in enumerate(val_loader):\n",
    "            \n",
    "            image= sample['image']\n",
    "            mask=sample['mask'].type(torch.LongTensor)\n",
    "            n_samples = image.size(0)\n",
    "            \n",
    "            mask = mask.cuda(non_blocking=True)\n",
    "            \n",
    "            labels = torch.argmax(mask, dim=1)\n",
    "            labels = labels.cuda()\n",
    "            image = image.cuda()\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "            # compute output\n",
    "            output = model(image)\n",
    "            output = output.cuda()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            accuracy = get_accuracy(output, labels)\n",
    "\n",
    "            losses.update(loss.item(), n_samples)\n",
    "            accuracies.update(accuracy)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            gc.collect()\n",
    "            del labels, image, mask\n",
    "        log_info(' * Accuracy: {}'.format(accuracies.avg))\n",
    "        log_info(' * Loss: {}'.format(losses.avg))\n",
    "    return accuracies.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    if not os.path.exists(OPTIONS.CHECKPOINT.DIR):\n",
    "        log_info(\"Creating dir '{}'...\".format(OPTIONS.CHECKPOINT.DIR))\n",
    "        os.makedirs(OPTIONS.CHECKPOINT.DIR)\n",
    "        log_info(\"Done.\")\n",
    "        \n",
    "    checkpoint_path = os.path.join(OPTIONS.CHECKPOINT.DIR, filename)\n",
    "    torch.save(state, checkpoint_path)\n",
    "    log_info(\"Checkpoint was saved to '{}'\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3,3)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = MultiStepLR(\n",
    "    optimizer, \n",
    "    OPTIONS.TRAIN.LR_DECAY_MILESTONES,                    \n",
    "    gamma=OPTIONS.TRAIN.LR_DECAY_GAMMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tlr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/opt/anaconda/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/3125]\tTime: 5.888 (5.888)\tData: 0.186\tAccuracy: 0.464\t\tLoss: 1.052194 (1.052194)\n",
      "Epoch: [1][50/3125]\tTime: 0.337 (0.443)\tData: 0.031\tAccuracy: 0.615\t\tLoss: 0.981316 (0.894671)\n",
      "Epoch: [1][100/3125]\tTime: 0.340 (0.390)\tData: 0.033\tAccuracy: 0.695\t\tLoss: 0.834702 (0.868566)\n",
      "Epoch: [1][150/3125]\tTime: 0.342 (0.374)\tData: 0.033\tAccuracy: 0.674\t\tLoss: 0.816043 (0.847997)\n",
      "Epoch: [1][200/3125]\tTime: 0.340 (0.365)\tData: 0.033\tAccuracy: 0.711\t\tLoss: 0.807860 (0.828714)\n",
      "Epoch: [1][250/3125]\tTime: 0.342 (0.361)\tData: 0.033\tAccuracy: 0.779\t\tLoss: 0.715131 (0.811818)\n",
      "Epoch: [1][300/3125]\tTime: 0.343 (0.358)\tData: 0.033\tAccuracy: 0.759\t\tLoss: 0.678405 (0.799044)\n",
      "Epoch: [1][350/3125]\tTime: 0.342 (0.355)\tData: 0.029\tAccuracy: 0.757\t\tLoss: 0.685873 (0.784436)\n",
      "Epoch: [1][400/3125]\tTime: 0.344 (0.354)\tData: 0.033\tAccuracy: 0.833\t\tLoss: 0.571442 (0.772005)\n",
      "Epoch: [1][450/3125]\tTime: 0.347 (0.353)\tData: 0.032\tAccuracy: 0.733\t\tLoss: 0.685467 (0.760835)\n",
      "Epoch: [1][500/3125]\tTime: 0.342 (0.352)\tData: 0.031\tAccuracy: 0.805\t\tLoss: 0.575319 (0.750335)\n",
      "Epoch: [1][550/3125]\tTime: 0.342 (0.351)\tData: 0.031\tAccuracy: 0.786\t\tLoss: 0.592531 (0.739471)\n",
      "Epoch: [1][600/3125]\tTime: 0.342 (0.350)\tData: 0.033\tAccuracy: 0.730\t\tLoss: 0.651284 (0.729474)\n",
      "Epoch: [1][650/3125]\tTime: 0.344 (0.350)\tData: 0.032\tAccuracy: 0.760\t\tLoss: 0.647815 (0.720526)\n",
      "Epoch: [1][700/3125]\tTime: 0.343 (0.349)\tData: 0.033\tAccuracy: 0.757\t\tLoss: 0.647300 (0.711512)\n",
      "Epoch: [1][750/3125]\tTime: 0.346 (0.349)\tData: 0.031\tAccuracy: 0.811\t\tLoss: 0.603360 (0.702875)\n",
      "Epoch: [1][800/3125]\tTime: 0.346 (0.349)\tData: 0.030\tAccuracy: 0.824\t\tLoss: 0.536451 (0.694983)\n",
      "Epoch: [1][850/3125]\tTime: 0.345 (0.348)\tData: 0.029\tAccuracy: 0.777\t\tLoss: 0.586926 (0.687198)\n",
      "Epoch: [1][900/3125]\tTime: 0.344 (0.348)\tData: 0.032\tAccuracy: 0.754\t\tLoss: 0.597484 (0.679562)\n",
      "Epoch: [1][950/3125]\tTime: 0.344 (0.348)\tData: 0.032\tAccuracy: 0.774\t\tLoss: 0.585610 (0.672828)\n",
      "Epoch: [1][1000/3125]\tTime: 0.346 (0.348)\tData: 0.033\tAccuracy: 0.823\t\tLoss: 0.495877 (0.666891)\n",
      "Epoch: [1][1050/3125]\tTime: 0.345 (0.348)\tData: 0.033\tAccuracy: 0.792\t\tLoss: 0.527916 (0.660425)\n",
      "Epoch: [1][1100/3125]\tTime: 0.342 (0.347)\tData: 0.029\tAccuracy: 0.766\t\tLoss: 0.557776 (0.654656)\n",
      "Epoch: [1][1150/3125]\tTime: 0.347 (0.347)\tData: 0.032\tAccuracy: 0.782\t\tLoss: 0.564852 (0.648385)\n",
      "Epoch: [1][1200/3125]\tTime: 0.343 (0.347)\tData: 0.032\tAccuracy: 0.768\t\tLoss: 0.572701 (0.642768)\n",
      "Epoch: [1][1250/3125]\tTime: 0.343 (0.347)\tData: 0.029\tAccuracy: 0.773\t\tLoss: 0.559176 (0.637564)\n",
      "Epoch: [1][1300/3125]\tTime: 0.345 (0.347)\tData: 0.032\tAccuracy: 0.826\t\tLoss: 0.465797 (0.632444)\n",
      "Epoch: [1][1350/3125]\tTime: 0.346 (0.347)\tData: 0.029\tAccuracy: 0.760\t\tLoss: 0.575826 (0.627833)\n",
      "Epoch: [1][1400/3125]\tTime: 0.344 (0.347)\tData: 0.031\tAccuracy: 0.814\t\tLoss: 0.488850 (0.623459)\n",
      "Epoch: [1][1450/3125]\tTime: 0.343 (0.347)\tData: 0.031\tAccuracy: 0.821\t\tLoss: 0.476933 (0.619155)\n",
      "Epoch: [1][1500/3125]\tTime: 0.346 (0.347)\tData: 0.032\tAccuracy: 0.833\t\tLoss: 0.460105 (0.614533)\n",
      "Epoch: [1][1550/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.826\t\tLoss: 0.455487 (0.610769)\n",
      "Epoch: [1][1600/3125]\tTime: 0.344 (0.346)\tData: 0.031\tAccuracy: 0.787\t\tLoss: 0.522645 (0.606710)\n",
      "Epoch: [1][1650/3125]\tTime: 0.347 (0.346)\tData: 0.032\tAccuracy: 0.869\t\tLoss: 0.379114 (0.602515)\n",
      "Epoch: [1][1700/3125]\tTime: 0.346 (0.346)\tData: 0.031\tAccuracy: 0.826\t\tLoss: 0.439595 (0.598930)\n",
      "Epoch: [1][1750/3125]\tTime: 0.347 (0.346)\tData: 0.032\tAccuracy: 0.762\t\tLoss: 0.560094 (0.595488)\n",
      "Epoch: [1][1800/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.814\t\tLoss: 0.476921 (0.592124)\n",
      "Epoch: [1][1850/3125]\tTime: 0.345 (0.346)\tData: 0.031\tAccuracy: 0.768\t\tLoss: 0.568330 (0.589013)\n",
      "Epoch: [1][1900/3125]\tTime: 0.346 (0.346)\tData: 0.032\tAccuracy: 0.815\t\tLoss: 0.468179 (0.585554)\n",
      "Epoch: [1][1950/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.845\t\tLoss: 0.403130 (0.582485)\n",
      "Epoch: [1][2000/3125]\tTime: 0.345 (0.346)\tData: 0.031\tAccuracy: 0.831\t\tLoss: 0.438617 (0.579028)\n",
      "Epoch: [1][2050/3125]\tTime: 0.347 (0.346)\tData: 0.032\tAccuracy: 0.841\t\tLoss: 0.407967 (0.575644)\n",
      "Epoch: [1][2100/3125]\tTime: 0.346 (0.346)\tData: 0.033\tAccuracy: 0.791\t\tLoss: 0.517718 (0.573015)\n",
      "Epoch: [1][2150/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.847\t\tLoss: 0.395145 (0.570158)\n",
      "Epoch: [1][2200/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.840\t\tLoss: 0.419686 (0.567355)\n",
      "Epoch: [1][2250/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.850\t\tLoss: 0.389771 (0.564869)\n",
      "Epoch: [1][2300/3125]\tTime: 0.345 (0.346)\tData: 0.032\tAccuracy: 0.843\t\tLoss: 0.419985 (0.562151)\n",
      "Epoch: [1][2350/3125]\tTime: 0.348 (0.346)\tData: 0.032\tAccuracy: 0.801\t\tLoss: 0.491284 (0.559707)\n",
      "Epoch: [1][2400/3125]\tTime: 0.348 (0.346)\tData: 0.032\tAccuracy: 0.805\t\tLoss: 0.495709 (0.557513)\n",
      "Epoch: [1][2450/3125]\tTime: 0.344 (0.346)\tData: 0.032\tAccuracy: 0.821\t\tLoss: 0.448385 (0.555142)\n",
      "Epoch: [1][2500/3125]\tTime: 0.344 (0.346)\tData: 0.029\tAccuracy: 0.833\t\tLoss: 0.417268 (0.553217)\n",
      "Epoch: [1][2550/3125]\tTime: 0.346 (0.346)\tData: 0.033\tAccuracy: 0.808\t\tLoss: 0.493328 (0.551200)\n",
      "Epoch: [1][2600/3125]\tTime: 0.347 (0.346)\tData: 0.033\tAccuracy: 0.824\t\tLoss: 0.432379 (0.548890)\n",
      "Epoch: [1][2650/3125]\tTime: 0.348 (0.346)\tData: 0.033\tAccuracy: 0.836\t\tLoss: 0.431063 (0.546712)\n",
      "Epoch: [1][2700/3125]\tTime: 0.341 (0.346)\tData: 0.030\tAccuracy: 0.831\t\tLoss: 0.411628 (0.544643)\n",
      "Epoch: [1][2750/3125]\tTime: 0.347 (0.346)\tData: 0.033\tAccuracy: 0.872\t\tLoss: 0.343735 (0.542528)\n",
      "Epoch: [1][2800/3125]\tTime: 0.347 (0.346)\tData: 0.029\tAccuracy: 0.886\t\tLoss: 0.316903 (0.540398)\n",
      "Epoch: [1][2850/3125]\tTime: 0.345 (0.346)\tData: 0.029\tAccuracy: 0.804\t\tLoss: 0.482678 (0.538476)\n",
      "Epoch: [1][2900/3125]\tTime: 0.347 (0.346)\tData: 0.030\tAccuracy: 0.814\t\tLoss: 0.456880 (0.536672)\n",
      "Epoch: [1][2950/3125]\tTime: 0.345 (0.346)\tData: 0.030\tAccuracy: 0.806\t\tLoss: 0.488187 (0.534762)\n",
      "Epoch: [1][3000/3125]\tTime: 0.348 (0.346)\tData: 0.029\tAccuracy: 0.840\t\tLoss: 0.413751 (0.532985)\n",
      "Epoch: [1][3050/3125]\tTime: 0.346 (0.346)\tData: 0.030\tAccuracy: 0.861\t\tLoss: 0.355761 (0.531197)\n",
      "Epoch: [1][3100/3125]\tTime: 0.342 (0.346)\tData: 0.029\tAccuracy: 0.833\t\tLoss: 0.419728 (0.529349)\n",
      " * Accuracy: 0.8386880155569448\n",
      " * Loss: 0.4085937828540802\n",
      "Checkpoint was saved to '/home/kevin/nuclei_segmentation/checkpoints/checkpoints_UNET_3class_64_DO20/epoch1.pth'\n",
      "Epoch: 2\tlr: 0.0001\n",
      "Epoch: [2][0/3125]\tTime: 0.388 (0.388)\tData: 0.238\tAccuracy: 0.825\t\tLoss: 0.466796 (0.466796)\n",
      "Epoch: [2][50/3125]\tTime: 0.347 (0.344)\tData: 0.029\tAccuracy: 0.819\t\tLoss: 0.458752 (0.427692)\n",
      "Epoch: [2][100/3125]\tTime: 0.344 (0.344)\tData: 0.029\tAccuracy: 0.851\t\tLoss: 0.382912 (0.425721)\n",
      "Epoch: [2][150/3125]\tTime: 0.346 (0.344)\tData: 0.029\tAccuracy: 0.781\t\tLoss: 0.603170 (0.421648)\n",
      "Epoch: [2][200/3125]\tTime: 0.344 (0.344)\tData: 0.029\tAccuracy: 0.857\t\tLoss: 0.363040 (0.417057)\n",
      "Epoch: [2][250/3125]\tTime: 0.343 (0.344)\tData: 0.029\tAccuracy: 0.841\t\tLoss: 0.397986 (0.419643)\n",
      "Epoch: [2][300/3125]\tTime: 0.346 (0.344)\tData: 0.029\tAccuracy: 0.864\t\tLoss: 0.379366 (0.418225)\n",
      "Epoch: [2][350/3125]\tTime: 0.347 (0.344)\tData: 0.029\tAccuracy: 0.873\t\tLoss: 0.332235 (0.417998)\n",
      "Epoch: [2][400/3125]\tTime: 0.345 (0.344)\tData: 0.029\tAccuracy: 0.868\t\tLoss: 0.348023 (0.420141)\n",
      "Epoch: [2][450/3125]\tTime: 0.346 (0.344)\tData: 0.029\tAccuracy: 0.821\t\tLoss: 0.469844 (0.420765)\n",
      "Epoch: [2][500/3125]\tTime: 0.346 (0.344)\tData: 0.030\tAccuracy: 0.786\t\tLoss: 0.519897 (0.420659)\n",
      "Epoch: [2][550/3125]\tTime: 0.344 (0.344)\tData: 0.029\tAccuracy: 0.829\t\tLoss: 0.427196 (0.420559)\n",
      "Epoch: [2][600/3125]\tTime: 0.342 (0.344)\tData: 0.029\tAccuracy: 0.848\t\tLoss: 0.387757 (0.420625)\n",
      "Epoch: [2][650/3125]\tTime: 0.344 (0.344)\tData: 0.029\tAccuracy: 0.826\t\tLoss: 0.450916 (0.420395)\n",
      "Epoch: [2][700/3125]\tTime: 0.346 (0.344)\tData: 0.029\tAccuracy: 0.846\t\tLoss: 0.391353 (0.419476)\n",
      "Epoch: [2][750/3125]\tTime: 0.345 (0.344)\tData: 0.029\tAccuracy: 0.855\t\tLoss: 0.373210 (0.418975)\n",
      "Epoch: [2][800/3125]\tTime: 0.344 (0.344)\tData: 0.029\tAccuracy: 0.826\t\tLoss: 0.426032 (0.418348)\n",
      "Epoch: [2][850/3125]\tTime: 0.346 (0.344)\tData: 0.029\tAccuracy: 0.863\t\tLoss: 0.345722 (0.418399)\n",
      "Epoch: [2][900/3125]\tTime: 0.344 (0.344)\tData: 0.032\tAccuracy: 0.808\t\tLoss: 0.488340 (0.417809)\n",
      "Epoch: [2][950/3125]\tTime: 0.346 (0.345)\tData: 0.031\tAccuracy: 0.828\t\tLoss: 0.421585 (0.417591)\n",
      "Epoch: [2][1000/3125]\tTime: 0.347 (0.345)\tData: 0.032\tAccuracy: 0.835\t\tLoss: 0.410357 (0.417049)\n",
      "Epoch: [2][1050/3125]\tTime: 0.341 (0.345)\tData: 0.032\tAccuracy: 0.867\t\tLoss: 0.343142 (0.416667)\n",
      "Epoch: [2][1100/3125]\tTime: 0.345 (0.345)\tData: 0.032\tAccuracy: 0.881\t\tLoss: 0.312584 (0.416287)\n",
      "Epoch: [2][1150/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.837\t\tLoss: 0.390217 (0.416232)\n",
      "Epoch: [2][1200/3125]\tTime: 0.346 (0.345)\tData: 0.033\tAccuracy: 0.861\t\tLoss: 0.345151 (0.416318)\n",
      "Epoch: [2][1250/3125]\tTime: 0.344 (0.345)\tData: 0.030\tAccuracy: 0.851\t\tLoss: 0.377512 (0.415786)\n",
      "Epoch: [2][1300/3125]\tTime: 0.348 (0.345)\tData: 0.030\tAccuracy: 0.859\t\tLoss: 0.346886 (0.415248)\n",
      "Epoch: [2][1350/3125]\tTime: 0.345 (0.345)\tData: 0.033\tAccuracy: 0.797\t\tLoss: 0.501999 (0.415279)\n",
      "Epoch: [2][1400/3125]\tTime: 0.346 (0.345)\tData: 0.030\tAccuracy: 0.836\t\tLoss: 0.419627 (0.415023)\n",
      "Epoch: [2][1450/3125]\tTime: 0.345 (0.345)\tData: 0.030\tAccuracy: 0.848\t\tLoss: 0.378020 (0.414675)\n",
      "Epoch: [2][1500/3125]\tTime: 0.344 (0.345)\tData: 0.032\tAccuracy: 0.828\t\tLoss: 0.458531 (0.414251)\n",
      "Epoch: [2][1550/3125]\tTime: 0.346 (0.345)\tData: 0.032\tAccuracy: 0.853\t\tLoss: 0.369032 (0.413788)\n",
      "Epoch: [2][1600/3125]\tTime: 0.344 (0.345)\tData: 0.032\tAccuracy: 0.844\t\tLoss: 0.393980 (0.413218)\n",
      "Epoch: [2][1650/3125]\tTime: 0.345 (0.345)\tData: 0.032\tAccuracy: 0.840\t\tLoss: 0.403117 (0.412574)\n",
      "Epoch: [2][1700/3125]\tTime: 0.346 (0.345)\tData: 0.033\tAccuracy: 0.835\t\tLoss: 0.423151 (0.412303)\n",
      "Epoch: [2][1750/3125]\tTime: 0.345 (0.345)\tData: 0.032\tAccuracy: 0.853\t\tLoss: 0.380436 (0.411962)\n",
      "Epoch: [2][1800/3125]\tTime: 0.346 (0.345)\tData: 0.032\tAccuracy: 0.842\t\tLoss: 0.385767 (0.411542)\n",
      "Epoch: [2][1850/3125]\tTime: 0.346 (0.345)\tData: 0.032\tAccuracy: 0.865\t\tLoss: 0.349401 (0.410909)\n",
      "Epoch: [2][1900/3125]\tTime: 0.346 (0.345)\tData: 0.033\tAccuracy: 0.877\t\tLoss: 0.314963 (0.410498)\n",
      "Epoch: [2][1950/3125]\tTime: 0.347 (0.345)\tData: 0.032\tAccuracy: 0.854\t\tLoss: 0.372781 (0.410317)\n",
      "Epoch: [2][2000/3125]\tTime: 0.348 (0.345)\tData: 0.032\tAccuracy: 0.819\t\tLoss: 0.462827 (0.409543)\n",
      "Epoch: [2][2050/3125]\tTime: 0.344 (0.345)\tData: 0.036\tAccuracy: 0.843\t\tLoss: 0.390895 (0.409185)\n",
      "Epoch: [2][2100/3125]\tTime: 0.344 (0.345)\tData: 0.033\tAccuracy: 0.894\t\tLoss: 0.282920 (0.408837)\n",
      "Epoch: [2][2150/3125]\tTime: 0.344 (0.345)\tData: 0.032\tAccuracy: 0.843\t\tLoss: 0.399904 (0.408524)\n",
      "Epoch: [2][2200/3125]\tTime: 0.342 (0.345)\tData: 0.032\tAccuracy: 0.825\t\tLoss: 0.446398 (0.408328)\n",
      "Epoch: [2][2250/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.849\t\tLoss: 0.379778 (0.407968)\n",
      "Epoch: [2][2300/3125]\tTime: 0.346 (0.345)\tData: 0.029\tAccuracy: 0.854\t\tLoss: 0.372185 (0.407688)\n",
      "Epoch: [2][2350/3125]\tTime: 0.342 (0.345)\tData: 0.029\tAccuracy: 0.825\t\tLoss: 0.429606 (0.407542)\n",
      "Epoch: [2][2400/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.853\t\tLoss: 0.382264 (0.406932)\n",
      "Epoch: [2][2450/3125]\tTime: 0.346 (0.345)\tData: 0.029\tAccuracy: 0.829\t\tLoss: 0.409945 (0.406526)\n",
      "Epoch: [2][2500/3125]\tTime: 0.344 (0.345)\tData: 0.029\tAccuracy: 0.822\t\tLoss: 0.445739 (0.405946)\n",
      "Epoch: [2][2550/3125]\tTime: 0.344 (0.345)\tData: 0.029\tAccuracy: 0.866\t\tLoss: 0.366670 (0.405631)\n",
      "Epoch: [2][2600/3125]\tTime: 0.347 (0.345)\tData: 0.029\tAccuracy: 0.877\t\tLoss: 0.318644 (0.405095)\n",
      "Epoch: [2][2650/3125]\tTime: 0.344 (0.345)\tData: 0.029\tAccuracy: 0.882\t\tLoss: 0.323771 (0.404590)\n",
      "Epoch: [2][2700/3125]\tTime: 0.346 (0.345)\tData: 0.029\tAccuracy: 0.801\t\tLoss: 0.475991 (0.404890)\n",
      "Epoch: [2][2750/3125]\tTime: 0.344 (0.345)\tData: 0.030\tAccuracy: 0.864\t\tLoss: 0.341077 (0.404686)\n",
      "Epoch: [2][2800/3125]\tTime: 0.346 (0.345)\tData: 0.033\tAccuracy: 0.788\t\tLoss: 0.489177 (0.404208)\n",
      "Epoch: [2][2850/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.844\t\tLoss: 0.392131 (0.404059)\n",
      "Epoch: [2][2900/3125]\tTime: 0.347 (0.345)\tData: 0.032\tAccuracy: 0.838\t\tLoss: 0.390188 (0.403841)\n",
      "Epoch: [2][2950/3125]\tTime: 0.345 (0.345)\tData: 0.033\tAccuracy: 0.821\t\tLoss: 0.441633 (0.403433)\n",
      "Epoch: [2][3000/3125]\tTime: 0.347 (0.345)\tData: 0.029\tAccuracy: 0.876\t\tLoss: 0.313101 (0.403375)\n",
      "Epoch: [2][3050/3125]\tTime: 0.343 (0.345)\tData: 0.032\tAccuracy: 0.878\t\tLoss: 0.320115 (0.403047)\n",
      "Epoch: [2][3100/3125]\tTime: 0.346 (0.345)\tData: 0.032\tAccuracy: 0.841\t\tLoss: 0.393351 (0.402891)\n",
      " * Accuracy: 0.8613098833126763\n",
      " * Loss: 0.34598990087509157\n",
      "Checkpoint was saved to '/home/kevin/nuclei_segmentation/checkpoints/checkpoints_UNET_3class_64_DO20/epoch2.pth'\n",
      "Epoch: 3\tlr: 5e-05\n",
      "Epoch: [3][0/3125]\tTime: 0.372 (0.372)\tData: 0.218\tAccuracy: 0.829\t\tLoss: 0.440796 (0.440796)\n",
      "Epoch: [3][50/3125]\tTime: 0.343 (0.345)\tData: 0.030\tAccuracy: 0.856\t\tLoss: 0.352775 (0.389819)\n",
      "Epoch: [3][100/3125]\tTime: 0.344 (0.345)\tData: 0.033\tAccuracy: 0.813\t\tLoss: 0.451229 (0.385518)\n",
      "Epoch: [3][150/3125]\tTime: 0.345 (0.345)\tData: 0.032\tAccuracy: 0.867\t\tLoss: 0.334749 (0.384050)\n",
      "Epoch: [3][200/3125]\tTime: 0.343 (0.345)\tData: 0.031\tAccuracy: 0.844\t\tLoss: 0.403974 (0.385107)\n",
      "Epoch: [3][250/3125]\tTime: 0.346 (0.345)\tData: 0.030\tAccuracy: 0.841\t\tLoss: 0.392777 (0.384399)\n",
      "Epoch: [3][300/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.830\t\tLoss: 0.422190 (0.382557)\n",
      "Epoch: [3][350/3125]\tTime: 0.345 (0.345)\tData: 0.033\tAccuracy: 0.867\t\tLoss: 0.335208 (0.381066)\n",
      "Epoch: [3][400/3125]\tTime: 0.346 (0.345)\tData: 0.030\tAccuracy: 0.858\t\tLoss: 0.353896 (0.381104)\n",
      "Epoch: [3][450/3125]\tTime: 0.348 (0.345)\tData: 0.033\tAccuracy: 0.891\t\tLoss: 0.280740 (0.379596)\n",
      "Epoch: [3][500/3125]\tTime: 0.345 (0.345)\tData: 0.029\tAccuracy: 0.826\t\tLoss: 0.445965 (0.378842)\n",
      "Epoch: [3][550/3125]\tTime: 0.344 (0.345)\tData: 0.033\tAccuracy: 0.864\t\tLoss: 0.357082 (0.379152)\n",
      "Epoch: [3][600/3125]\tTime: 0.346 (0.345)\tData: 0.033\tAccuracy: 0.854\t\tLoss: 0.356617 (0.378321)\n",
      "Epoch: [3][650/3125]\tTime: 0.345 (0.346)\tData: 0.031\tAccuracy: 0.844\t\tLoss: 0.409555 (0.377824)\n",
      "Epoch: [3][700/3125]\tTime: 0.346 (0.346)\tData: 0.030\tAccuracy: 0.863\t\tLoss: 0.346696 (0.377929)\n",
      "Epoch: [3][750/3125]\tTime: 0.347 (0.346)\tData: 0.031\tAccuracy: 0.868\t\tLoss: 0.335694 (0.377624)\n",
      "Epoch: [3][800/3125]\tTime: 0.349 (0.346)\tData: 0.033\tAccuracy: 0.862\t\tLoss: 0.363373 (0.377749)\n",
      "Epoch: [3][850/3125]\tTime: 0.345 (0.346)\tData: 0.030\tAccuracy: 0.845\t\tLoss: 0.391290 (0.377902)\n",
      "Epoch: [3][900/3125]\tTime: 0.344 (0.346)\tData: 0.033\tAccuracy: 0.821\t\tLoss: 0.440086 (0.377858)\n",
      "Epoch: [3][950/3125]\tTime: 0.343 (0.346)\tData: 0.033\tAccuracy: 0.866\t\tLoss: 0.347763 (0.377740)\n",
      "Epoch: [3][1000/3125]\tTime: 0.345 (0.346)\tData: 0.030\tAccuracy: 0.861\t\tLoss: 0.352516 (0.377397)\n",
      "Epoch: [3][1050/3125]\tTime: 0.347 (0.346)\tData: 0.033\tAccuracy: 0.885\t\tLoss: 0.291965 (0.377732)\n",
      "Epoch: [3][1100/3125]\tTime: 0.346 (0.346)\tData: 0.032\tAccuracy: 0.867\t\tLoss: 0.341469 (0.377668)\n",
      "Epoch: [3][1150/3125]\tTime: 0.348 (0.346)\tData: 0.033\tAccuracy: 0.860\t\tLoss: 0.346717 (0.377783)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, OPTIONS.TRAIN.MAX_EPOCH+1):\n",
    "    lr_scheduler.step(epoch)\n",
    "    log_info('Epoch: {}\\tlr: {}'.format(epoch, lr_scheduler.get_lr()[0]))\n",
    "    train(train_loader, model, criterion, optimizer, epoch=epoch)\n",
    "    validate(val_loader, model, criterion)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.module.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, 'epoch{}.pth'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
